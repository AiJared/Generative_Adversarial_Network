{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxqtSFqfzy6flP2c5M1FbK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AiJared/Generative_Adversarial_Network/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A8AaSZbceDhE"
      },
      "outputs": [],
      "source": [
        " import tensorflow as tf\n",
        " import numpy as np\n",
        " import matplotlib.pyplot as plt\n",
        " import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageGenerationGAN:\n",
        "    def __init__(self, img_shape=(64, 64, 3), latent_dim=100):\n",
        "        \"\"\"\n",
        "        Initialize GAN with specified image dimensions and latent space\n",
        "\n",
        "        Args:\n",
        "            img_shape (tuple): Dimensions of input images\n",
        "            latent_dim (int): Dimensionality of the random noise vector\n",
        "        \"\"\"\n",
        "        self.img_shape = img_shape\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Compile generator and discriminator\n",
        "        self.generator = self.build_generator()\n",
        "        self.discriminator = self.build_discriminator()\n",
        "\n",
        "        # Compile the adversarial model\n",
        "        self.adversarial_model = self.build_gan()\n",
        "\n",
        "    def build_generator(self):\n",
        "        \"\"\"\n",
        "        Build the generator network\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: Generator neural network\n",
        "        \"\"\"\n",
        "        model = tf.keras.Sequential([\n",
        "            # Input layer\n",
        "            tf.keras.layers.Dense(8 * 8 * 256, input_dim=self.latent_dim),\n",
        "            tf.keras.layers.Reshape((8, 8, 256)),\n",
        "\n",
        "            # Upsampling blocks\n",
        "            tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "            tf.keras.layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "            tf.keras.layers.Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='tanh')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        \"\"\"\n",
        "        Build the discriminator network\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: Discriminator neural network\n",
        "        \"\"\"\n",
        "        model = tf.keras.Sequential([\n",
        "            # Input layer\n",
        "            tf.keras.layers.Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=self.img_shape),\n",
        "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "            tf.keras.layers.Conv2D(128, kernel_size=4, strides=2, padding='same'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def build_gan(self):\n",
        "        \"\"\"\n",
        "        Build the Generative Adversarial Network\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: Combined GAN model\n",
        "        \"\"\"\n",
        "        # Set discriminator to non-trainable when training generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Connect generator and discriminator\n",
        "        model = tf.keras.Sequential([\n",
        "            self.generator,\n",
        "            self.discriminator\n",
        "        ])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        \"\"\"\n",
        "        Single training step for the GAN\n",
        "\n",
        "        Args:\n",
        "            real_images (tf.Tensor): Batch of real training images\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        # Generate noise\n",
        "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
        "\n",
        "        # Generate fake images\n",
        "        generated_images = self.generator(noise, training=True)\n",
        "\n",
        "        # Prepare labels\n",
        "        real_labels = tf.ones((batch_size, 1))\n",
        "        fake_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train Discriminator\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "            real_predictions = self.discriminator(real_images, training=True)\n",
        "            fake_predictions = self.discriminator(generated_images, training=True)\n",
        "\n",
        "            disc_real_loss = tf.keras.losses.binary_crossentropy(real_labels, real_predictions)\n",
        "            disc_fake_loss = tf.keras.losses.binary_crossentropy(fake_labels, fake_predictions)\n",
        "\n",
        "            disc_loss = 0.5 * (disc_real_loss + disc_fake_loss)\n",
        "\n",
        "        # Compute gradients and update discriminator\n",
        "        disc_gradients = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "        tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5).apply_gradients(\n",
        "            zip(disc_gradients, self.discriminator.trainable_variables)\n",
        "        )\n",
        "\n",
        "        # Train Generator\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            noise = tf.random.normal([batch_size, self.latent_dim])\n",
        "            generated_images = self.generator(noise, training=True)\n",
        "\n",
        "            fake_predictions = self.discriminator(generated_images, training=True)\n",
        "            gen_loss = tf.keras.losses.binary_crossentropy(real_labels, fake_predictions)\n",
        "\n",
        "        # Compute gradients and update generator\n",
        "        gen_gradients = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "        tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5).apply_gradients(\n",
        "            zip(gen_gradients, self.generator.trainable_variables)\n",
        "        )\n",
        "\n",
        "    def train(self, dataset, epochs=100, batch_size=32):\n",
        "        \"\"\"\n",
        "        Train the GAN on the provided dataset\n",
        "\n",
        "        Args:\n",
        "            dataset (tf.data.Dataset): Training dataset\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            for batch in dataset:\n",
        "                self.train_step(batch)\n",
        "\n",
        "            # Optional: Generate and save sample images\n",
        "            if epoch % 10 == 0:\n",
        "                self.generate_and_save_images(epoch)\n",
        "\n",
        "    def generate_and_save_images(self, epoch, num_examples=16):\n",
        "        \"\"\"\n",
        "        Generate sample images during training\n",
        "\n",
        "        Args:\n",
        "            epoch (int): Current training epoch\n",
        "            num_examples (int): Number of images to generate\n",
        "        \"\"\"\n",
        "        noise = tf.random.normal([num_examples, self.latent_dim])\n",
        "        generated_images = self.generator(noise, training=False)\n",
        "\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for i in range(num_examples):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            plt.imshow((generated_images[i] + 1) / 2.0)  # Rescale to [0,1]\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.savefig(f'generated_images_epoch_{epoch}.png')\n",
        "        plt.close()\n",
        "\n",
        "def preprocess_images(image_directory, target_size=(64, 64)):\n",
        "    \"\"\"\n",
        "    Load and preprocess images from a directory\n",
        "\n",
        "    Args:\n",
        "        image_directory (str): Path to image directory\n",
        "        target_size (tuple): Desired image dimensions\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: Preprocessed image dataset\n",
        "    \"\"\"\n",
        "    image_paths = [os.path.join(image_directory, f) for f in os.listdir(image_directory)\n",
        "                   if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    def load_and_preprocess_image(path):\n",
        "        image = tf.io.read_file(path)\n",
        "        image = tf.image.decode_image(image, channels=3)\n",
        "        image = tf.image.resize(image, target_size)\n",
        "        image = tf.cast(image, tf.float32)\n",
        "        image = (image / 127.5) - 1  # Normalize to [-1, 1]\n",
        "        return image\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    dataset = dataset.map(load_and_preprocess_image)\n",
        "    dataset = dataset.batch(32)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "7suZPRQbefjS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    # Load your custom dataset\n",
        "    dataset = preprocess_images('path/to/your/image/directory')\n",
        "\n",
        "    # Initialize and train the GAN\n",
        "    gan = ImageGenerationGAN(img_shape=(64, 64, 3))\n",
        "    gan.train(dataset, epochs=2)"
      ],
      "metadata": {
        "id": "e1sCH7aijTDl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}